{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Önişleme, Öznitelik Çıkartımı ve *Pipeline* İşlevselliği\n",
    "\n",
    "Şimdiye kadar mevcut veri setlerini yükledik ve doğrudan kullandık. Ancak gerçek problemlerde çoğu zaman veriyi çeşitli önişlemlerden geçirmek, doğru formata sokmak (scikit-learn için `[nokta_sayisi oznitelik_sayisi]` şeklinde bir 2 boyutlu dizin), her algoritma için geçerli olmasa da sayısal değerlere çevirmek (kategorik veriler, yazı vb.) ve/veya veriden öznitelik çıkartmak gereklidir. Scikit-learn kütühanesinde bu gereksinimler için bir takım işlevler mevcuttur. Bu işlevler ana olarak \n",
    "`preprocessing`, `feature_extraction` ve `pipeline` modülleri içerisindedir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ortamı hazırlayalım\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ölçeklendirme (Scaling)\n",
    "\n",
    "Verilerin boyutları farklı ölçeklerde olabilir. Bu durumun öğrenme algoritmasını etkilememesi için verilerin belirli standardizasyondan geçirilmesi önerilir. Ölçeklendirme için yöntemler:\n",
    "\n",
    "* Z-Normalization: Verinin her boyutunu 0 ortalama ve 1 standart sapma olacak şekilde düzenlemek. Scikit\n",
    "* Min-Max Scaling: Verinin her boyutunu iki değer arasında kalacak şekilde düzenlemek. Genelde bu aralık [0,1] ya da [-1,1] olarak seçilir.\n",
    "* Max Absolute Value Scaling: Verinin her boyutunun mutlak değerinin belirli bir değerden küçük olacak şekilde düzenlemek. Genelde bu değer 1 olur. Sadece çarpma işlemi olduğu için aralıklı veriler ile kullanılması uygundur.\n",
    "* Robust Scaling: Veride fazla sayıda **aykırı** değer normal ölçeklendirme işlemlerini kötü etkileyebilir. Medyan ve çeyrek genişlik değerleri kullanan bir ölçekleme bu tip veriler için uygundur.\n",
    "\n",
    "Uyarı: Çok fazla 0 içeren aralıklı verilerin (sparse data) önişlem adımlarından geçirilecekse dikkat edilmelidir. Aksi takdirde seyrek yapıları bozulabilir. Önişleme yapılırken uygun yaklaşımlar seçilmelidir. En uygun yöntemler, merkezleme yapmayan yöntemlerdir.\n",
    "\n",
    "Yapay öğrenme uygulamalarında ölçeklendirme bilgilerini (örneğin hangi değeri ekledik/çıkardık ve hangi değer ile çarptık/böldük) tutmak isteriz. Bunun nedeni modelin yürürlüğe alındığı zaman gelen verilerin de aynı şekilde ölçeklendirme gereksinimidir.\n",
    "\n",
    "Şimdi bir kaç örnek görelim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.],\n",
    "              [ 1.,  0.,  1.]])\n",
    "print('Orjinal:')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xzn = preprocessing.scale(X)\n",
    "print('Z-Normalized:')\n",
    "print(Xzn)\n",
    "print()\n",
    "\n",
    "print('Ortalama ve Standart Sapma')\n",
    "print(Xzn.mean(axis=0),Xzn.std(axis=0))\n",
    "print()\n",
    "\n",
    "#Ölçekleme bilgisini saklayacak şekilde:\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Değerleri hesapla\n",
    "scaler.fit(X) \n",
    "print('Ölçekleme yapacak sınıf:')\n",
    "print(scaler)\n",
    "print('Ölçekleme değerleri:')\n",
    "print(scaler.mean_, scaler.scale_)\n",
    "print('Z-Normalized:')\n",
    "#Gelen veriyi ölçeklendir. İlk veri olmasına gerek yok\n",
    "print(scaler.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#Değerleri hesapla ve hemen orjinal datayı ölçeklendir\n",
    "Xmm = min_max_scaler.fit_transform(X)\n",
    "print('[0,1] aralığına:')\n",
    "print(Xmm)\n",
    "print()\n",
    "print('Ölçekleme değerleri:')\n",
    "print(min_max_scaler.scale_, min_max_scaler.min_)\n",
    "Y = np.array([[1.5, -1.,1.5]])\n",
    "print('Başka veri ölçekleme')\n",
    "print(Y)\n",
    "print(scaler.transform(Y))\n",
    "print()\n",
    "\n",
    "print('[-1,1] aralığına')\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "print(preprocessing.MinMaxScaler(feature_range=[-1,1]).fit_transform(X))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutlak Değer Ölçeklemesi (Sparse Veriler için uygun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "Xma = max_abs_scaler.fit_transform(X)\n",
    "print('Mutlak Değer 1e:')\n",
    "print(Xmm)\n",
    "print()\n",
    "print('Ölçekleme değeri:')\n",
    "print(max_abs_scaler.scale_)\n",
    "print()\n",
    "\n",
    "print('Başka veri ölçekleme')\n",
    "Y = np.array([[ -3., -1.,  4.]])\n",
    "print(Y)\n",
    "print(max_abs_scaler.transform(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aykırı Değerlerle Ölçekleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ölçeklendirme il alakasız veri yaratılıyor:\n",
    "Xo = np.random.standard_t(2,(20,2))\n",
    "plt.boxplot(Xo)\n",
    "plt.title('Veri')\n",
    "down, up = plt.ylim()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "z_scaler = preprocessing.StandardScaler()\n",
    "Xz = z_scaler.fit_transform(Xo)\n",
    "plt.boxplot(Xz)\n",
    "plt.title('Z-Normalized')\n",
    "plt.ylim(down,up)\n",
    "plt.show()\n",
    "\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "Xrs = robust_scaler.fit_transform(Xo)\n",
    "plt.boxplot(Xrs)\n",
    "plt.title('Robust')\n",
    "plt.ylim(down,up)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bazı veri türleri veya problemler için boyutların teker teker ölçeklenmesi uygun değildir. Verilerin beraber ölçeklenmesi gerekebilir. Buna örnek olarak veri ağartması (whitening) verilebilir. Başka bir örnek ise, verinin birim norma sahip olacak şekilde düzenlenmesidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Orjinal:')\n",
    "print(X)\n",
    "print()\n",
    "\n",
    "Xn = preprocessing.normalize(X, norm='l2')\n",
    "print('Birim Vektöre Normalize:')\n",
    "print(Xn)  \n",
    "print()\n",
    "\n",
    "#Bu işlemin belirli bir değeri yoktur, gelen veriye göre yapılır. \n",
    "#Ancak aynı işleveselliği sürdürmesi için, sınıf versiyonu da mevcuttur\n",
    "normalizer = preprocessing.Normalizer()\n",
    "print(normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kategorik Verilerin Kodlanması"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir çok öğrenme algoritması, girdi olarak sayı değeleri (vektör, matris vb. dahil) bekler. Bu yüzden kategorik verileri bu şekilde kodlamak gereklidir. Mesela verimiz müşteri ile ilgili bilgiler olsun: `[cinsiyet, şehir, meslek]`. Cinsiyet için `[kadın, erkek]`, şehir için `[ankara, istanbul, izmir]` ve meslek `[özel, kamu, serbest, emekli]` seçenekleri olsun. \n",
    "\n",
    "Bildiğimiz üzere doğrudan tamsayı vermek iyi bir fikir değil. Bu yüzden *one-hot* kodlaması yapıyoruz. Scikit-learn'de:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "M = [['erkek', 'ankara', 'kamu'], \n",
    "     ['kadın', 'istanbul', 'özel'],\n",
    "     ['kadın', 'izmir', 'emekli'],\n",
    "     ['erkek', 'istanbul', 'serbest']]\n",
    "#Kategorileri veriden çıkartarak\n",
    "enc.fit(M)\n",
    "print('Kodlayıcı:')\n",
    "print(enc)\n",
    "print('Veriden kodlamaya:')\n",
    "# 2 (cinsiyet) + 3 (şehir) + 4(meslek) = 9 boyutlu\n",
    "print(enc.transform([['kadın', 'istanbul', 'serbest'],\n",
    "                     ['erkek', 'ankara', 'emekli']]).toarray())\n",
    "print('Kategoriler:')\n",
    "print(enc.categories_)\n",
    "print()\n",
    "#Kategorileri el ile vererek\n",
    "cinsiyet = ['kadın', 'erkek'] \n",
    "sehir = ['ankara', 'istanbul', 'izmir']\n",
    "meslek = ['özel', 'kamu', 'serbest', 'emekli']\n",
    "enc2 = preprocessing.OneHotEncoder(categories=[cinsiyet, sehir, meslek])\n",
    "print('Kodlayıcı:')\n",
    "print(enc2)\n",
    "#Yine de fit çağırmamız gerekli\n",
    "enc2.fit(M)\n",
    "print('Veriden kodlamaya:')\n",
    "print(enc2.transform([['kadın', 'istanbul', 'serbest'],\n",
    "                     ['erkek', 'ankara', 'emekli']]).toarray())\n",
    "print()\n",
    "\n",
    "#Eğer beklenmedik kategorik veri gelme ihtimali varsa:\n",
    "enc3 = preprocessing.OneHotEncoder(handle_unknown='ignore')\n",
    "enc3.fit(M) \n",
    "print('Kodlayıcı:')\n",
    "print(enc3)\n",
    "print('Veride bulunmayan kategorik veri gelince hepsi 0 olan uygun boyutta bir vektör atıyor:')\n",
    "print(enc3.transform([['kadın', 'Bursa', 'kamu']]).toarray())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veriler bazen dictionary veri yapıları içinde gelebilir. Bunun için:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dictData = [\n",
    "    {'fiyat': 1200000, 'oda': 4, 'mahalle': 'Maslak'},\n",
    "    {'fiyat': 1400000, 'oda': 3, 'mahalle': 'Etiler'},\n",
    "    {'fiyat':  500000, 'oda': 3, 'mahalle': 'Tuzla'},\n",
    "    {'fiyat':  900000, 'oda': 2, 'mahalle': 'Etiler'}]\n",
    "vec = DictVectorizer(sparse=False, dtype=int)\n",
    "print(vec.fit_transform(dictData))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksik Verilerin Atanması (Imputation)\n",
    "\n",
    "Gerçek dünyada verilerin bazı boyutları eksik gelebilir. Bunla başa çıkmanın değişik yolları vardır. En basit ve yaygınca kullanılan yöntemler arasında ortalama, medyan, mod veya sabit bir değer kullanmak gelir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "#Ortalama\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6], [5, 3], [4, 4]])       \n",
    "\n",
    "print('Ortalama:')\n",
    "Xe = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(Xe))        \n",
    "\n",
    "imp2 = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp2.fit([[1, 2], [np.nan, 3], [7, 6]]) \n",
    "print('Medyan:')\n",
    "print(imp2.transform(Xe))\n",
    "\n",
    "print('Pandastaki Kategorikler üzerinde en sık:')\n",
    "df = pd.DataFrame([[\"a\", \"x\"],\n",
    "                   [np.nan, \"y\"],\n",
    "                   [\"a\", np.nan],\n",
    "                   [\"b\", \"y\"]], dtype=\"category\")\n",
    "\n",
    "imp3 = SimpleImputer(strategy=\"most_frequent\")\n",
    "print(imp3.fit_transform(df))\n",
    "\n",
    "print('Pandastaki Kategorikler üzerinde sabit:')\n",
    "imp4 = SimpleImputer(strategy=\"constant\", fill_value = 'c')\n",
    "print(imp4.fit_transform(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eksik veriler, doğrusal regresyon ve korelasyon bilgileri kullanılarak da doldurulabilirler. Bu yöntemler daha iyi çalışmaktadır. Ancak şu an itibariyle scikit-learn kütüphanesinde henüz bulunmamaktadırlar. Yakın zamanda `IterativeImputer` adıyla geleceklerdir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polinom Öznitelikler\n",
    "\n",
    "Veriye doğrusalsızlık (non-linearity) eklemek, öğrenme açısından faydalı olabilir. Bunun en kolay yöntemlerinden birisi de mevcut verilerin yüksek dereceli ve etkileşimli polinom değerlerini veriye eklemektir. Bunu `PolynomialFeatures` modülü ile yapmak mümkün."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "Xp = np.arange(6).reshape(3, 2)\n",
    "print('Orjinal:')\n",
    "print(Xp)\n",
    "poly = PolynomialFeatures(2)\n",
    "print('Polinom Öznitelikleri Eklenmiş: 1 x1 x2 (x1)^2 x1*x2 (x2)^2')\n",
    "print(poly.fit_transform(Xp))\n",
    "poly2 = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "print('Sadece etkileşim Polinom Öznitelikleri Eklenmiş: x1 x2 x1x2')\n",
    "print(poly2.fit_transform(Xp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Özel Önişleme veya Öznitelik Uygulaması\n",
    "\n",
    "Veri temizlemek, önişlemek veya öznitelik çıkartmak için bazen özel fonksiyonlar tanımlanmak istenebilir. Bunu yapmanın birçok yolu vardır ancak birazdan açık olacak nedenler için (Pipeline işlevi) aşağıdaki şekilde tanımlanması önerilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "Xt = np.array([[0, 1], [2, 3]])\n",
    "print(Xt)\n",
    "print(np.log1p(Xt))\n",
    "print(transformer.transform(Xt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yazı Öznitelikleri\n",
    "\n",
    "Yazı verisi ile öğrenme yapmak için vektöre çevrilmeleri gerekir. Bunun için literatürde birçok yöntem vardır. Scikit-learn görece geri kalmış ancak yine de kullanışlı iki öznitelik çıkartma yöntemini destekler. Bunun ilki doğrudan kelime sayılarını vektöre çeviren *bag-of-words* modeli, diğeri de sayıları genel kullanım ile dengeleyen *tf-idf* modelidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "ornek = ['bugun hava sicak', 'sicak sicak corbani ic', 'hava su yol elektrik', 'cok yol yaptim bugun']\n",
    "\n",
    "vec = CountVectorizer()\n",
    "Xbow = vec.fit_transform(ornek)\n",
    "display(pd.DataFrame(Xbow.toarray(), columns=vec.get_feature_names()))\n",
    "\n",
    "vec2 = TfidfVectorizer()\n",
    "Xtfidf = vec2.fit_transform(ornek)\n",
    "display(pd.DataFrame(Xtfidf.toarray(), columns=vec.get_feature_names()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu öznitelikleri nasıl kullanacağımızı birazdan göreceğiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline: İşlemleri ve Tahminleyicileri Uç Uca Eklemek\n",
    "\n",
    "Tipik bir yapay öğrenme uygulamasında, verinin üzerinde bir takım işlemler (önişleme, öznitelik hesaplama, öznitelik ekleme, veriyi dönüştürme vb.) yapıldıktan sonra veri öğrenme algoritmasına verilir. Örnek olarak: \n",
    "1. Eksik verileri ortalamalar ile kapat\n",
    "2. Veriyi [-1,1] arasına ölçekle\n",
    "3. İkinci dereceden polinom öznitelikleri ekle\n",
    "4. Veriden öğren.\n",
    "\n",
    "Burada veri bir takım **dönüşümden** geçtikten sonra öğrenme sürecine giriyor. Giriş kısmında, öğrenme algoritmalarının kullandığı Estimator API'yi görmüştük. Benzer bir şekilde dönüştürme algoritmaları için de Transformer API var. Bu API `fit()`, `transform()` ve ikisinin tek bir adımda yapıldığı `fit_transform()` fonskiyonlarını tanımlıyor. \n",
    "\n",
    "Scikit-learn bu süreci soyutlaştırmak ve kolaylaştırmak için `pipeline` işlevselliğini tanımlanış. Bu soyutlaştırma, birden çok transformerdan geçen verinin, en son hali ile bir estimatora girmesini temsil ediyor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pipe = make_pipeline(SimpleImputer(missing_values=np.nan, strategy='mean'),\n",
    "                     preprocessing.MinMaxScaler(),\n",
    "                     PolynomialFeatures(degree=2),\n",
    "                     LinearRegression())\n",
    "print(pipe)\n",
    "\n",
    "#eğer her adıma isim vermek istersek:\n",
    "from sklearn.pipeline import Pipeline\n",
    "steps = [('impute', SimpleImputer(missing_values=np.nan, strategy='mean')), \n",
    "         ('scale', preprocessing.MinMaxScaler()),\n",
    "         ('poly', PolynomialFeatures(degree=2)),\n",
    "         ('learn', LinearRegression())]\n",
    "pipe2 = Pipeline(steps)\n",
    "print(pipe2)\n",
    "\n",
    "Xpp = np.array([[ np.nan, 0,   3  ],\n",
    "              [ 3,   7,   9  ],\n",
    "              [ 3,   5,   2  ],\n",
    "              [ 4,   np.nan, 6  ],\n",
    "              [ 8,   8,   1  ]])\n",
    "\n",
    "ypp = np.array([14.2, 15.9, -1.01,  7.93, -5.2])\n",
    "pipe.fit(Xpp, ypp) \n",
    "print()\n",
    "print(ypp)\n",
    "print(pipe.predict(Xpp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
