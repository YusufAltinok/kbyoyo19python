{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresyon\n",
    "\n",
    "Tikhonov, Polinom, Basis Function\n",
    "Validasyon Görselleştirmesi\n",
    "Gradient Tree Boosting, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ortamı hazırlayalım\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doğrusal regresyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "x = np.linspace(-1,1,50)\n",
    "X = x[:,np.newaxis]\n",
    "y = 2*x+1 + np.random.normal(0,0.25,len(x))\n",
    "lr = LinearRegression(fit_intercept = True)\n",
    "lr.fit(X,y)\n",
    "ylr = lr.predict(X)\n",
    "plt.scatter(X,y)\n",
    "plt.plot(X,ylr,'r')\n",
    "plt.show()\n",
    "\n",
    "print('Ortalama Karesel Hata: ', mean_squared_error(y,ylr))\n",
    "print('R2 puanı: ', lr.score(X,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn kestirim için tasarlanmış. O yüzden istatistiksel hesaplamalara girmiyor. Ancak regresyon parametrelerinin güven aralıkları öznitelik seçimi için kullanılabilir. Bu özelliğe ihtiyaç varsa bir yol gerekli kodu yazmak. Ancak **statsmodel** kütüphanesinde bu özellik mevcut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polinom Regresyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Pipeline adımlarını hazırlayalım. 5. dereceden seçelim\n",
    "# include_bias=True yazıp, fit_intercept=False da olabilirdi\n",
    "stepsPoly = [('poly', PolynomialFeatures(5, include_bias=False)), \n",
    "             ('lr', LinearRegression(fit_intercept=True)) ]\n",
    "\n",
    "#make_pipeline ile de yapabilirdik\n",
    "pipePoly = Pipeline(stepsPoly)\n",
    "xP = np.linspace(0.1,7,100)    \n",
    "#Normal gürültü yerine tekdüze (uniform) gürültü yaratıp biraz daha zorlayalım\n",
    "yP = np.log(xP) + np.sin(xP) + np.random.uniform(-0.5,0.5,len(xP))#np.random.normal(0,0.25,len(xP))\n",
    "             \n",
    "XP = xP[:,np.newaxis]\n",
    "pipePoly.fit(XP,yP)\n",
    "print('R2: ', pipePoly.score(XP,yP))\n",
    "yPredP = pipePoly.predict(XP)\n",
    "\n",
    "plt.scatter(xP,yP)\n",
    "plt.plot(xP,yPredP,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veri polinom olmasa da öğrenebiliyor. Ancak veri görmediğimiz yerden gelseydi bu iyi çalışmaz, o yüzden dikkatli olmak gerekli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xP2 = np.linspace(6,10,100)\n",
    "yP2 = np.log(xP2) + np.sin(xP2)\n",
    "XP2 = xP2[:,np.newaxis]\n",
    "yPredP2 = pipePoly.predict(XP2)\n",
    "plt.plot(xP2,yP2)\n",
    "plt.plot(xP2,yPredP2,'r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Başka Fonksiyonlarla Regresyon\n",
    "\n",
    "Polinomlar dışında da fonksiyon kullanmak mümkün. Gaussian RBF özniteliklerine bakalım. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GaussianRbfFeatures():\n",
    "    def __init__(self,merkezSayisi=10,genlikKatsayisi=1.0):\n",
    "        #Merkezleri ve genlikleri veriye göre otomatik ayarlıyoruz\n",
    "        self.k = merkezSayisi\n",
    "        self.h = genlikKatsayisi\n",
    "    \n",
    "    @staticmethod\n",
    "    def _rbf(x,c,h):\n",
    "        return np.exp(-np.sum(((x-c)/h)**2, axis=1))\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        self.merkezler_ = np.linspace(X.min(), X.max(), self.k)\n",
    "        self.genlikler_ = self.h*(self.merkezler_[1]-self.merkezler_[0])\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        return self._rbf(X[:, :, np.newaxis], self.merkezler_, self.genlikler_)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X,y)\n",
    "        return self.transform(X)\n",
    "        \n",
    "stepsRBF = [('rbf', GaussianRbfFeatures(20)), \n",
    "            ('lr', LinearRegression(fit_intercept=True)) ]\n",
    "\n",
    "#make_pipeline ile de yapabilirdik\n",
    "pipeRBF = Pipeline(stepsRBF)\n",
    "             \n",
    "pipeRBF.fit(XP,yP)\n",
    "print('R2: ', pipeRBF.score(XP,yP))\n",
    "yPredRBF = pipeRBF.predict(XP)\n",
    "\n",
    "plt.scatter(xP,yP)\n",
    "plt.plot(xP,yPredRBF,'r')\n",
    "plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kendi tanımladığımız öznitelikler üzerinden de hiperparametre arayabiliriz. Ancak bunun öznitelik çıkartan transformerımıza bir takım değişikler yapmamız gerekli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GaussianRbfFeaturesPipeline(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,merkezSayisi=10,genlikKatsayisi=1.0):\n",
    "        #Merkezleri ve genlikleri veriye göre otomatik ayarlıyoruz\n",
    "        self.k = merkezSayisi\n",
    "        self.h = genlikKatsayisi\n",
    "    \n",
    "    @staticmethod\n",
    "    def _rbf(x,c,h):\n",
    "        return np.exp(-np.sum(((x-c)/h)**2, axis=1))\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        self.merkezler_ = np.linspace(X.min(), X.max(), self.k)\n",
    "        self.genlikler_ = self.h*(self.merkezler_[1]-self.merkezler_[0])\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        return self._rbf(X[:, :, np.newaxis], self.merkezler_, self.genlikler_)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X,y)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'merkezSayisi':self.k,'genlikKatsayisi':self.h}\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        self.k = params['merkezSayisi']\n",
    "        self.h = params['genlikKatsayisi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "paramsRBF = {'rbf__merkezSayisi': np.arange(5,20), \n",
    "             'rbf__genlikKatsayisi': np.linspace(0.25,5.0,100)}\n",
    "stepsRBF2 = [('rbf', GaussianRbfFeaturesPipeline()), \n",
    "             ('lr', LinearRegression(fit_intercept=True)) ]\n",
    "\n",
    "pipeRBF2 = Pipeline(stepsRBF2)\n",
    "\n",
    "rgbCV = RandomizedSearchCV(pipeRBF2, paramsRBF, cv=cv, n_iter=50)\n",
    "rgbCV.fit(XP,yP)\n",
    "print('R2: ', rgbCV.score(XP,yP))\n",
    "yPredRBF2 = rgbCV.predict(XP)\n",
    "print(rgbCV.best_params_ )\n",
    "plt.scatter(xP,yP)\n",
    "plt.plot(xP,yPredRBF2,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Öznitelikler ile doğrusalsızlık ekleyerek daha iyi sonuçlar alabiliyoruz. Ancak burada overfitting durumuna dikkat etmemiz gerekiyor. Bunu hiperparametre taraması yaparak azalttık. Ancak bu tarama süreci vakit alabilir. Öznitelik tanımlamadan bile overfitting mümkün. \n",
    "\n",
    "Overfittingi özniteliklerden bağımsız bir şekilde regülarizasyon ile azaltabiliriz. Bunun bir yolu *Ridge Regression*. Eğitim performansını düşürse de overfittinge karşı daha başarılı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "pipeRBF3 = make_pipeline(GaussianRbfFeatures(50,2.), \n",
    "                         LinearRegression(fit_intercept=True))\n",
    "pipeRBF3.fit(XP, yP)\n",
    "print('R2: ', pipeRBF3.score(XP,yP))\n",
    "yPredRBF3 =  pipeRBF3.predict(XP)\n",
    "plt.scatter(xP,yP)\n",
    "plt.plot(xP,yPredRBF3,'r')\n",
    "plt.show()\n",
    "\n",
    "print(pipeRBF3.named_steps['linearregression'].coef_, pipeRBF3.named_steps['linearregression'].intercept_ )\n",
    "\n",
    "pipeRbfRidge = make_pipeline(GaussianRbfFeatures(50,2.), \n",
    "                             Ridge(alpha=2.0,fit_intercept=True))\n",
    "pipeRbfRidge.fit(XP, yP)\n",
    "print('R2: ', pipeRbfRidge.score(XP,yP))\n",
    "yPredRbfRidge =  pipeRbfRidge.predict(XP)\n",
    "plt.scatter(xP,yP)\n",
    "plt.plot(xP,yPredRbfRidge,'r')\n",
    "plt.show()\n",
    "\n",
    "print(pipeRbfRidge.named_steps['ridge'].coef_, pipeRbfRidge.named_steps['ridge'].intercept_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir de Lasso regresyonu var. Bu yöntem, bazı öznitelikleri 0 olmaya zorluyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "pipeRbfLasso = make_pipeline(GaussianRbfFeatures(50,2.), \n",
    "                             Lasso(alpha=0.001, fit_intercept=True))\n",
    "pipeRbfLasso.fit(XP, yP)\n",
    "print('R2: ', pipeRbfLasso.score(XP,yP))\n",
    "yPredRbfLasso =  pipeRbfLasso.predict(XP)\n",
    "plt.scatter(xP,yP)\n",
    "plt.plot(xP,yPredRbfLasso,'r')\n",
    "plt.show()\n",
    "\n",
    "print(pipeRbfLasso.named_steps['lasso'].coef_, pipeRbfLasso.named_steps['lasso'].intercept_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn içinde başka regresyon algoritmaları da mevcut. Bunlardan bir kaçını scikit-learn içindeki boston veri seti ile deneyelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "seed = int(time.time())\n",
    "\n",
    "def quickTest(X, y, clf):\n",
    "    #Her algoritma aynı veri setini görsün diye\n",
    "    np.random.seed(seed)\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.4)\n",
    "    clf.fit(Xtrain, ytrain)\n",
    "    ypred = clf.predict(Xtest)\n",
    "    print(type(clf).__name__ + ': ', mean_squared_error(ypred,ytest))\n",
    "    return type(clf).__name__, mean_squared_error(ypred,ytest)\n",
    "\n",
    "\n",
    "boston_data = datasets.load_boston()\n",
    "X = boston_data['data']\n",
    "y = boston_data['target']\n",
    "\n",
    "names = ['']*5\n",
    "errors = [0]*5\n",
    "\n",
    "print('Düşük daha iyi:')\n",
    "names[0], errors[0] = quickTest(X,y,LinearRegression(fit_intercept=True))\n",
    "names[1], errors[1] = quickTest(X,y,RandomForestRegressor(n_estimators=100))\n",
    "names[2], errors[2] = quickTest(X,y,MLPRegressor(hidden_layer_sizes=(30,), max_iter=10000))\n",
    "names[3], errors[3] = quickTest(X,y,GaussianProcessRegressor(kernel=DotProduct()+WhiteKernel()))\n",
    "names[4], errors[4] = quickTest(X,y,GradientBoostingRegressor())\n",
    "\n",
    "plt.bar(list(range(5)),errors)\n",
    "plt.xticks(list(range(5)),names,rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
